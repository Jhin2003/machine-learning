{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NU CCMACLRL - Multiple Linear Regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovgxH-F4UOme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad689e1d-571f-4a50-878b-eeaa8da5fb40"
      },
      "source": [
        "!pip install scikit-learn==0.24"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==0.24 in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ0fbOd-QFEa"
      },
      "source": [
        "# Multiple Linear Regression\n",
        "***\n",
        "\n",
        "- Multiple linear regression differs from the simple one because it can handle multiple input features\n",
        "- It is a simple algorithm initially developed in the field of statistics and was studied as a model for understanding the relationship between input and output variables\n",
        "- It is a linear model - assumes linear relationship between input variables (X) and the output variable (y)\n",
        "- Used to predict continious values (e.g., weight, price...)\n",
        "\n",
        "\n",
        "#### Assumptions\n",
        "1. **Linear Assumption** — model assumes the relationship between variables is linear\n",
        "2. **No Noise** — model assumes that the input and output variables are not noisy — so remove outliers if possible\n",
        "3. **No Collinearity** — model will overfit when you have highly correlated input variables\n",
        "4. **Normal Distribution** — the model will make more reliable predictions if your input and output variables are normally distributed. If that’s not the case, try using some transforms on your variables to make them more normal-looking\n",
        "5. **Rescaled Inputs** — use scalers or normalizer to make more reliable predictions\n",
        "\n",
        "#### Take-home point\n",
        "- Training a linear regression model means calculating the best coefficients for the line equation formula\n",
        "- The best coefficients can be obtained with **gradient descent**\n",
        "    - An iterative optimization algorithm that calculates derivatives wrt. each coefficient, and updates the coefficients on the go\n",
        "    - One additional parameter - *learning rate*, specifies the rate at which coefficients are updated\n",
        "        - High learning rate can lead to \"missing\" the best values\n",
        "        - Low learning rate can lead to slow optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJhNAsvaQFEe"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Math behind\n",
        "- In a nutshell, we need to calculate an array of weights and a single bias through gradient descent\n",
        "- Weights = slopes, bias = y intercept\n",
        "- We're still solving a simple line equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7E6NcprQFEf"
      },
      "source": [
        "$$ \\large \\hat{y} = wx + b $$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Multiple** linear regression.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/414/1*Ko7YDmTa_TctiL2Fkm-kGQ.png\">\n",
        "\n",
        "<br>\n",
        "\n",
        "- We also need a **cost function**\n",
        "    - Function we want to minimize\n",
        "    - Reducing the cost (loss) leads to better equation parameters\n",
        "    - We'll choose Mean Squared Error (MSE), but feel free to work with any other\n",
        "\n",
        "$$ \\large MSE = \\frac{1}{N} \\sum_{i=1}^{n} ((y_i - \\hat{y})^2 $$\n",
        "\n",
        "<br>\n",
        "\n",
        "- $ \\hat{y} $ can be further written into:\n",
        "\n",
        "$$ \\large MSE = \\frac{1}{N} \\sum_{i=1}^{n} ((y_i - (wx_i + b))^2 $$\n",
        "\n",
        "<br>\n",
        "\n",
        "- To update weights and biases, we're using gradient descent\n",
        "- It relies on partial derivatives calculation for each parameter\n",
        "- Below you'll find derived MSE wrt. each parameter:\n",
        "\n",
        "$$ \\large \\partial_w = \\frac{1}{N} \\sum_{i=1}^{n} 2x_i(\\hat{y} - y) $$\n",
        "<br>\n",
        "$$ \\large \\partial_b = \\frac{1}{N} \\sum_{i=1}^{n} 2(\\hat{y} - y) $$\n",
        "\n",
        "<br>\n",
        "\n",
        "- This $ 2 $ can be ommited, or you can leave it - it's not important\n",
        "- Next, we're updating the existing weights and bias according to the following formulas:\n",
        "\n",
        "$$ \\large w = w - \\alpha \\cdot \\partial_w $$\n",
        "<br>\n",
        "$$ \\large b = b - \\alpha \\cdot \\partial_b $$\n",
        "\n",
        "- Where $ \\alpha $ is the learning rate\n",
        "- This process is then repeated for a predefined number of iterations\n",
        "- Let's see this in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLXRjF7hQFEf"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Implementation\n",
        "- You'll need only Numpy to implement the logic\n",
        "- Matplotlib is used for optional visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qti-wVOEQFEg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = (14, 7)\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4oFOKszQFEh"
      },
      "source": [
        "<br>\n",
        "\n",
        "- The `LinearRegression` class is written to follow the familiar Scikit-Learn syntax\n",
        "- The coefficients are set to `None` at the start - `__init__()` method\n",
        "- We're also keeping track of the loss (just for visualization purposes)\n",
        "- The `fit()` method calculates the coefficients\n",
        "- The `predict()` method essentially implements the line equation\n",
        "- The `_mean_squared_error()` private function is used to calculate loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWNjGJCkQFEh"
      },
      "source": [
        "class LinearRegression:\n",
        "    '''\n",
        "    A class which implements linear regression model with gradient descent.\n",
        "    '''\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=10000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights, self.bias = None, None\n",
        "        self.loss = []\n",
        "        \n",
        "    @staticmethod\n",
        "    def _mean_squared_error(y, y_hat):\n",
        "        '''\n",
        "        Private method, used to evaluate loss at each iteration.\n",
        "        \n",
        "        :param: y - array, true values\n",
        "        :param: y_hat - array, predicted values\n",
        "        :return: float\n",
        "        '''\n",
        "        error = 0\n",
        "        for i in range(len(y)):\n",
        "            error += (y[i] - y_hat[i]) ** 2\n",
        "        return error / len(y)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Used to calculate the coefficient of the linear regression model.\n",
        "        \n",
        "        :param X: array, features\n",
        "        :param y: array, true values\n",
        "        :return: None\n",
        "        '''\n",
        "        # 1. Initialize weights and bias to zeros\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "        self.bias = 0\n",
        "        \n",
        "        # 2. Perform gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Line equation\n",
        "            y_hat = np.dot(X, self.weights) + self.bias\n",
        "            loss = self._mean_squared_error(y, y_hat)\n",
        "            self.loss.append(loss)\n",
        "            \n",
        "            # Calculate derivatives\n",
        "            partial_w = (1 / X.shape[0]) * (2 * np.dot(X.T, (y_hat - y)))\n",
        "            partial_d = (1 / X.shape[0]) * (2 * np.sum(y_hat - y))\n",
        "            \n",
        "            # Update the coefficients\n",
        "            self.weights -= self.learning_rate * partial_w\n",
        "            self.bias -= self.learning_rate * partial_d\n",
        "        \n",
        "        \n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Makes predictions using the line equation.\n",
        "        \n",
        "        :param X: array, features\n",
        "        :return: array, predictions\n",
        "        '''\n",
        "        return np.dot(X, self.weights) + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnnTByBvQFEi"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Testing\n",
        "- We'll use the *diabetes* dataset from Scikit-Learn. Data source: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq2FeZJ0QFEi"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83yVxAlT1DF"
      },
      "source": [
        "- Let's look at the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiidUfUTT5ZI",
        "outputId": "5a78dca3-d4ec-4e56-eaff-4522cd8bf968"
      },
      "source": [
        "data_frame = load_diabetes(as_frame=True,  return_X_y=False)\n",
        "data_frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, T-Cells (a type of white blood cells)\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, thyroid stimulating hormone\\n      - s5      ltg, lamotrigine\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)',\n",
              " 'data':           age       sex       bmi  ...        s4        s5        s6\n",
              " 0    0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
              " 1   -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
              " 2    0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
              " 3   -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
              " 4    0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
              " ..        ...       ...       ...  ...       ...       ...       ...\n",
              " 437  0.041708  0.050680  0.019662  ... -0.002592  0.031193  0.007207\n",
              " 438 -0.005515  0.050680 -0.015906  ...  0.034309 -0.018118  0.044485\n",
              " 439  0.041708  0.050680 -0.015906  ... -0.011080 -0.046879  0.015491\n",
              " 440 -0.045472 -0.044642  0.039062  ...  0.026560  0.044528 -0.025930\n",
              " 441 -0.045472 -0.044642 -0.073030  ... -0.039493 -0.004220  0.003064\n",
              " \n",
              " [442 rows x 10 columns],\n",
              " 'data_filename': '/usr/local/lib/python3.7/dist-packages/sklearn/datasets/data/diabetes_data.csv.gz',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'frame':           age       sex       bmi  ...        s5        s6  target\n",
              " 0    0.038076  0.050680  0.061696  ...  0.019908 -0.017646   151.0\n",
              " 1   -0.001882 -0.044642 -0.051474  ... -0.068330 -0.092204    75.0\n",
              " 2    0.085299  0.050680  0.044451  ...  0.002864 -0.025930   141.0\n",
              " 3   -0.089063 -0.044642 -0.011595  ...  0.022692 -0.009362   206.0\n",
              " 4    0.005383 -0.044642 -0.036385  ... -0.031991 -0.046641   135.0\n",
              " ..        ...       ...       ...  ...       ...       ...     ...\n",
              " 437  0.041708  0.050680  0.019662  ...  0.031193  0.007207   178.0\n",
              " 438 -0.005515  0.050680 -0.015906  ... -0.018118  0.044485   104.0\n",
              " 439  0.041708  0.050680 -0.015906  ... -0.046879  0.015491   132.0\n",
              " 440 -0.045472 -0.044642  0.039062  ...  0.044528 -0.025930   220.0\n",
              " 441 -0.045472 -0.044642 -0.073030  ... -0.004220  0.003064    57.0\n",
              " \n",
              " [442 rows x 11 columns],\n",
              " 'target': 0      151.0\n",
              " 1       75.0\n",
              " 2      141.0\n",
              " 3      206.0\n",
              " 4      135.0\n",
              "        ...  \n",
              " 437    178.0\n",
              " 438    104.0\n",
              " 439    132.0\n",
              " 440    220.0\n",
              " 441     57.0\n",
              " Name: target, Length: 442, dtype: float64,\n",
              " 'target_filename': '/usr/local/lib/python3.7/dist-packages/sklearn/datasets/data/diabetes_target.csv.gz'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk8NKC57QFEj"
      },
      "source": [
        "- The below code cell applies train/test split to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Qsl1J0QFEk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVTDD0JXQFEk"
      },
      "source": [
        "- You can now initialize and train the model, and afterwards make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sffe4qEQFEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4705d752-bf66-4142-f1a7-881036e08733"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "model = SGDRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1223: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OZv783KQFEl"
      },
      "source": [
        "- These are the \"optimal\" weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sze90C1QFEl",
        "outputId": "5b33f663-0bfb-4287-9090-cbf544fc928d"
      },
      "source": [
        "model.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  56.18833323,  -41.39979092,  262.06089707,  187.45978128,\n",
              "         28.69566099,    4.00266732, -144.32990897,  134.23708812,\n",
              "        218.44767461,  132.3359013 ])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2Ph54UQFEm"
      },
      "source": [
        "- And this is the \"optimal\" bias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACPHCno1QFEm",
        "outputId": "811ad350-a66e-4ebf-a8da-1e70b60a4d3b"
      },
      "source": [
        "model.intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([152.25970031])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSrjmWBzQFEm"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Loss Evaluation\n",
        "\n",
        "- Let's visualize the loss at each iteration\n",
        "- Ideally, we should see a curve that starts high and soon drops to zero or near zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASDpuQ9VQFEn"
      },
      "source": [
        "losses = {}\n",
        "for lr in [0.5, 0.1, 0.01, 0.001]:\n",
        "    model = SGDRegressor(max_iter=lr)\n",
        "    model.fit(X_train, y_train)\n",
        "    losses[f'LR={str(lr)}'] = model.loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxEzNdDoQFEn"
      },
      "source": [
        "- You can now visualize loss at each iteration for these different learning rates:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBlcbqQiQFEo"
      },
      "source": [
        "- Seems like `learning_rate=0.5` works the best\n",
        "- Let's train the model with it and evalute (print MSE):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMW8oPX5QFEo",
        "outputId": "3c5c37f1-c4e5-4909-cd4c-dc27b280ecf2"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "model = SGDRegressor(max_iter=1000, eta0=0.0005, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "mean_squared_error(y_test, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1223: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5021.854976744705"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU70Y4Tcy8UG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For Asynchronous Session (Sep 18):\n",
        "\n",
        "1. Using the new diabetes dataset, what would be the optimum values of learning rate and epochs (# of iterations) to achieve the lowest possible MSE?\n",
        "\t\n",
        "2. Following results from question 1, what are the values of the parameters (B1-B10) and B0 (intercept) you obtained that best represents the model for the disease progression?\n",
        "\n",
        "3. Try applying multiple linear regression to a new dataset. Please find applicable datasets at [data.world](https://data.world/datasets/regression)\n"
      ]
    }
  ]
}